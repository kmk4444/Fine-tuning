{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGG5qXzH/T96gTf40xyF9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/Fine-tuning/blob/main/Fine_tuning_Llama_3_8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we check the GPU version available in the environment and install specific dependencies that are compatible with the detected GPU to prevent version conflicts."
      ],
      "metadata": {
        "id": "YKy-CxKD2-oc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TMPdU9ia0-RM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to prepare to load a range of quantized language models, including a new 15 trillion token LLama-3 model, optimized for memory efficiency with 4-bit quantization."
      ],
      "metadata": {
        "id": "wyetmvwB8EYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 #Choose any! Llama 3 is up to 8k\n",
        "dtype= None\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be false.\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2b-it-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "  model_name = \"unsloth/llama-3-8b-bnb-4bit\", #Llama-3 70b also works (just change the model name)\n",
        "  max_seq_length = max_seq_length,\n",
        "  dtype = dtype,\n",
        "  load_in_4bit = load_in_4bit,\n",
        "  # token = \"---\", #use one if using gated models like meta-llama/Lllama-2-7b-hf\n",
        " )"
      ],
      "metadata": {
        "id": "BS0elq_p8XnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to prepare to load a range of quantized language models, including a new 15 trillion token Lllama-3 model, optimized for memory efficiency with 4-bit quantization."
      ],
      "metadata": {
        "id": "3ZibdBDuQOKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # Choose any number >0 ! Suggested 8,16,32,64,128\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora= False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MldSe3kQyy1",
        "outputId": "915685fb-5255-4268-c343-8fc53d929c50"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep\n",
        "- We now use the Alpaca dataset from https://huggingface.co/datasets/yahma/alpaca-cleaned , which is a filtered version of 52K of the original https://crfm.stanford.edu/2023/03/13/alpaca.html . You can replace this code section with your own data prep.\n",
        "\n",
        "- Then, we define a system prompt that formats tasks into instructions, input and responses, and apply it to a dataset to prepare our inputs and outputs for the model, with an EOS token to signal completion.\n"
      ],
      "metadata": {
        "id": "1yMwabyVRau0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is basically the system prompt\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # do not forget this part!\n",
        "def formatting_prompts_func(examples):\n",
        "  instructions = examples[\"instruction\"]\n",
        "  inputs = examples[\"input\"]\n",
        "  outputs = examples[\"output\"]\n",
        "  texts = []\n",
        "  for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "    text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN # without this token generation goes on forever!\n",
        "    texts.append(text)\n",
        "  return {\"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "c58YOr9OSoF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "- We do 60 steps to speed things up, but you can set num_train_epochs=1 for a full run, and turn off max_steps=None.\n",
        "- At this stage, we're configuring our model's training setup, where we define things like batch size and learning rate, to teach our model effectively with the data we have prepared."
      ],
      "metadata": {
        "id": "ASd1-m4aUxe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = None, # increase this to make the model learn \"better\"\n",
        "        num_train_epochs=4\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "6CI8hUU4VY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show current memory stats"
      ],
      "metadata": {
        "id": "LSCO0ZjXXC-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f'GPU = {gpu_stats.name}. Max memory = {max_memory} GB.')\n",
        "print(f'{start}')"
      ],
      "metadata": {
        "id": "Xh3BekboXGLq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}